{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import  tqdm\n",
    "import GPUtil as GPU\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "import cv2\n",
    "from torchvision.transforms import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask=pd.read_csv('train_masks.csv')\n",
    "metadata=pd.read_csv('metadata.csv')\n",
    "submission=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images= os.listdir('train_hq')\n",
    "test_images= os.listdir('test_hq')\n",
    "train_masks= os.listdir('train_masks')\n",
    "train_path= [os.path.join('train',i) for i in os.listdir('train')]\n",
    "test_path=[os.path.join('test',i) for i in os.listdir('test')]\n",
    "mask_path=[os.path.join('train_masks',i) for i in os.listdir('train_masks')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6254.0\n"
     ]
    }
   ],
   "source": [
    "print(len(test_images)/16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 318 training images, and 6254 test images. No of training images>test images so that might create a problem. Valdimir approaches this by pseudo labelling, see what it is later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def plot_image_and_mask(train_images):\n",
    "    #plot the train_images\n",
    "    figure,axis=plt.subplots(len(train_images),2,figsize=(2*10,10*len(train_images)))\n",
    "    for i in range(len(train_images)):\n",
    "        id=train_images[i]\n",
    "        image_path=os.path.join('train',id)\n",
    "        mask_path=os.path.join('train_masks',id)[:-4]+'_mask.gif'\n",
    "        print(np.array(Image.open(image_path)).shape)\n",
    "        axis[i][0].imshow(Image.open(image_path))\n",
    "        axis[i][1].imshow(Image.open(mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_image_and_mask(train_images[50:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shutil craetes symbolic link to the directories where the files are copied. When we create folds, the data does not disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test code to create the folds \n",
    "# from pathlib import Path\n",
    "# import shutil\n",
    "# local_data_path=Path('.').absolute()/'folds'\n",
    "# local_data_path.mkdir(exist_ok=True)\n",
    "# train_path= Path('.').absolute()/'train_hq/'\n",
    "# #train_path= Path('.').absolute()/'train/'\n",
    "# mask_path=Path('.').absolute()/'train_masks'\n",
    "# train_file_list=train_images\n",
    "# folds=pd.read_csv('folds_csv.csv')\n",
    "# num_folds=folds.fold.nunique()\n",
    "# #the cars are at sixteen angles \n",
    "# angles=['0'+ str(i) for i in range (1,10)]+ [str(i) for i in range(10,17)]\n",
    "# #now create the folder containing the folds \n",
    "# for fold in range(num_folds):\n",
    "#     #create the folder correspoonding to number of folds\n",
    "#     (local_data_path / str(fold) / 'train' / 'images').mkdir(exist_ok=True, parents=True)\n",
    "#     (local_data_path / str(fold) / 'train' / 'masks').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "#     (local_data_path / str(fold) / 'val' / 'images').mkdir(exist_ok=True, parents=True)\n",
    "#     (local_data_path / str(fold) / 'val' / 'masks').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# for i in tqdm(folds.index):\n",
    "#     car_id=folds.loc[i,'id']\n",
    "#     fold=folds.loc[i,'fold']\n",
    "#     #copy the 16 images to fold directory \n",
    "#     #this is considered as a val this time \n",
    "#     for angle in angles:\n",
    "#         old_image_path = train_path / (car_id + '_' + angle + '.jpg')\n",
    "\n",
    "#         new_image_path = local_data_path / str(fold) / 'val' / 'images' / (car_id + '_' + angle + '.jpg')\n",
    "#         shutil.copy(str(old_image_path), str(new_image_path))\n",
    "\n",
    "#         old_mask_path = mask_path / (car_id + '_' + angle + '_mask.gif')\n",
    "#         new_mask_path = local_data_path / str(fold) / 'val' / 'masks' / (car_id + '_' + angle + '_mask.gif')\n",
    "#         shutil.copy(str(old_mask_path), str(new_mask_path))\n",
    "        \n",
    "#     #for all the other folds this will be considered as a  training image\n",
    "#     for t_fold in range(num_folds):\n",
    "#             if t_fold == fold:\n",
    "#                 continue\n",
    "\n",
    "#             for angle in angles:\n",
    "#                 old_image_path = train_path / (car_id + '_' + angle + '.jpg')\n",
    "\n",
    "#                 new_image_path = local_data_path / str(t_fold) / 'train' / 'images' / (car_id + '_' + angle + '.jpg')\n",
    "#                 shutil.copy(str(old_image_path), str(new_image_path))\n",
    "\n",
    "#                 old_mask_path = mask_path / (car_id + '_' + angle + '_mask.gif')\n",
    "#                 new_mask_path = local_data_path / str(t_fold) / 'train' / 'masks' / (car_id + '_' + angle + '_mask.gif')\n",
    "#                 shutil.copy(str(old_mask_path), str(new_mask_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take care of the modelling now. The original implementation used unet 11 with a pre-trained vgg-11 encoder, which is what we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "import torch \n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vladimir uses the Unet architecture for semantic segmentation. The modifications to the net are taurusunet where a pre-trained encoder is used for the segmentation. Let's try to do the modelling here and try to understand the input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle multiple gpus in the jupyter notebook \n",
    "device=torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "if device.type=='cuda':\n",
    "    gpu_id=[]\n",
    "    GPUs = GPU.getGPUs()\n",
    "    for i in range(len(GPUs)):\n",
    "        gpu = GPUs[i]\n",
    "        if gpu.memoryFree>5000:\n",
    "            gpu_id.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "def concat(xs):\n",
    "    return torch.cat(xs, 1)\n",
    "\n",
    "\n",
    "class Conv3BN(nn.Module):\n",
    "    def __init__(self, in_: int, out: int, bn=False):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.bn = nn.BatchNorm2d(out) if bn else None\n",
    "        self.activation = nn.SELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_: int, out: int):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet11(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=32):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.encoder = models.vgg11(pretrained=True).features\n",
    "        self.relu = self.encoder[1]\n",
    "        self.conv1 = self.encoder[0]\n",
    "        self.conv2 = self.encoder[3]\n",
    "        self.conv3s = self.encoder[6]\n",
    "        self.conv3 = self.encoder[8]\n",
    "        self.conv4s = self.encoder[11]\n",
    "        self.conv4 = self.encoder[13]\n",
    "        self.conv5s = self.encoder[16]\n",
    "        self.conv5 = self.encoder[18]\n",
    "\n",
    "        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n",
    "        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.relu(self.conv1(x))\n",
    "        conv2 = self.relu(self.conv2(self.pool(conv1)))\n",
    "        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n",
    "        conv3 = self.relu(self.conv3(conv3s))\n",
    "        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n",
    "        conv4 = self.relu(self.conv4(conv4s))\n",
    "        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n",
    "        conv5 = self.relu(self.conv5(conv5s))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "        return F.sigmoid(self.final(dec1))\n",
    "\n",
    "    \n",
    "#writing the dice based loss weights for the neural network \n",
    "\n",
    "class Loss:\n",
    "    #init script for the loss\n",
    "    def __init__(self, dice_weight=1):\n",
    "        self.nll_loss = nn.BCELoss()\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    #call called if parameters are given to the network \n",
    "    def __call__(self, outputs, targets):\n",
    "        loss = self.nll_loss(outputs, targets)\n",
    "        if self.dice_weight:\n",
    "            eps = 1e-15\n",
    "            dice_target = (targets == 1).float()\n",
    "            dice_output = outputs\n",
    "            intersection = (dice_output * dice_target).sum()\n",
    "            union = dice_output.sum() + dice_target.sum() + eps\n",
    "\n",
    "            loss -= torch.log(2 * intersection / union)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "#create a dataloader for the carvana dataset\n",
    "train_images= os.listdir('train_hq')\n",
    "test_images= os.listdir('test_hq')\n",
    "train_masks= os.listdir('train_masks')\n",
    "train_path= [os.path.join('train',i) for i in os.listdir('train')]\n",
    "test_path=[os.path.join('test',i) for i in os.listdir('test')]\n",
    "mask_path=[os.path.join('train_masks',i) for i in os.listdir('train_masks')]\n",
    "\n",
    "'''returns the padded image'''\n",
    "#writing the image loader function \n",
    "def load_image(path):\n",
    "    img=cv2.imread(path)\n",
    "    img = cv2.copyMakeBorder(img, 0, 0, 1, 1, cv2.BORDER_REFLECT_101)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img=cv2.resize(img,(img.shape[1]//2,img.shape[0]//2))\n",
    "    return img.astype(np.uint8)\n",
    "'''returns the mask'''\n",
    "def load_mask(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            if '.gif' in str(path):\n",
    "                img = (np.asarray(img) > 0)\n",
    "            else:\n",
    "                img = (np.asarray(img) > 255 * 0.5)\n",
    "            img = cv2.copyMakeBorder(img.astype(np.uint8), 0, 0, 1, 1, cv2.BORDER_REFLECT_101)\n",
    "            img=cv2.resize(img,(img.shape[1]//2,img.shape[0]//2))\n",
    "            return img.astype(np.float32)\n",
    "        \n",
    "img_transform = Compose([\n",
    "    ToTensor(),\n",
    "    #Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "        \n",
    "'''since we are feeding the images to pretrained vgg11, we need to normalize the weights\n",
    "https://pytorch.org/docs/stable/torchvision/models.html\n",
    "make a transformation pipeline for input images in the carvana dataset'''\n",
    "#our dataloader should also handle the image augmentation \n",
    "#inherit the Dataset\n",
    "\n",
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, root: Path, to_augment=False):\n",
    "        # TODO This potentially may lead to bug.\n",
    "        self.image_paths = sorted(root.joinpath('images').glob('*.jpg'))\n",
    "        self.mask_paths = sorted(root.joinpath('masks').glob('*'))\n",
    "        self.to_augment = to_augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img=load_image(str(self.image_paths[idx]))\n",
    "        mask = load_mask(str(self.mask_paths[idx]))\n",
    "#\n",
    "\n",
    "        return img_transform(img), torch.from_numpy(np.expand_dims(mask, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.9529, 0.9529, 0.9490,  ..., 0.9529, 0.9529, 0.9529],\n",
       "          [0.9529, 0.9529, 0.9490,  ..., 0.9529, 0.9529, 0.9529],\n",
       "          [0.9529, 0.9529, 0.9490,  ..., 0.9529, 0.9529, 0.9529],\n",
       "          ...,\n",
       "          [0.7020, 0.7020, 0.7020,  ..., 0.6941, 0.6980, 0.6980],\n",
       "          [0.6980, 0.6980, 0.6980,  ..., 0.6941, 0.6980, 0.6980],\n",
       "          [0.6980, 0.6980, 0.6980,  ..., 0.6941, 0.6941, 0.6941]],\n",
       " \n",
       "         [[0.9529, 0.9529, 0.9490,  ..., 0.9529, 0.9529, 0.9529],\n",
       "          [0.9529, 0.9529, 0.9490,  ..., 0.9529, 0.9529, 0.9529],\n",
       "          [0.9529, 0.9529, 0.9490,  ..., 0.9529, 0.9529, 0.9529],\n",
       "          ...,\n",
       "          [0.7176, 0.7176, 0.7176,  ..., 0.7020, 0.7059, 0.7059],\n",
       "          [0.7137, 0.7137, 0.7137,  ..., 0.7020, 0.7059, 0.7059],\n",
       "          [0.7137, 0.7137, 0.7137,  ..., 0.7020, 0.7020, 0.7020]],\n",
       " \n",
       "         [[0.9451, 0.9451, 0.9412,  ..., 0.9529, 0.9529, 0.9529],\n",
       "          [0.9451, 0.9451, 0.9412,  ..., 0.9529, 0.9529, 0.9529],\n",
       "          [0.9451, 0.9451, 0.9412,  ..., 0.9529, 0.9529, 0.9529],\n",
       "          ...,\n",
       "          [0.7216, 0.7216, 0.7216,  ..., 0.6980, 0.7020, 0.7020],\n",
       "          [0.7176, 0.7176, 0.7176,  ..., 0.6980, 0.7020, 0.7020],\n",
       "          [0.7176, 0.7176, 0.7176,  ..., 0.6980, 0.6980, 0.6980]]]),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_data_path=Path('.').absolute()/'folds'\n",
    "a=CarvanaDataset(local_data_path/str(2)/'train')\n",
    "fold_path=sorted((local_data_path/str(2)/'train').joinpath('images').glob('*.jpg'))\n",
    "fold_path[0]\n",
    "a[0]\n",
    "\n",
    "#load_image('/data2/6666/rajat.modi/practice/Carvana/carvana/CarvanaSegmentation/folds/2/train/images/00087a6bd4dc_01.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_loader(ds_root: Path, to_augment=False, shuffle=False):\n",
    "    return DataLoader(\n",
    "        dataset=CarvanaDataset(ds_root, to_augment=to_augment),\n",
    "        shuffle=shuffle,\n",
    "        num_workers=1,\n",
    "        batch_size=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "gpu_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bashturtle/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py:26: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    }
   ],
   "source": [
    "#create the model \n",
    "model=UNet11()\n",
    "criterion=Loss() \n",
    "model=nn.DataParallel(model,device_ids=gpu_id).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each fold, load its train and val sets\n",
    "fold_no=2\n",
    "train_loader=make_loader(local_data_path/str(fold_no)/'train')\n",
    "val_loader=make_loader(local_data_path/str(fold_no)/'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################VALIDATION FUNCTION#################\n",
    "def validation(model:nn.Module,criterion,valid_loader):\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    dice=[]\n",
    "    for inputs,target in valid_loader:\n",
    "        target=target.to(device)\n",
    "        output=model(inputs).to(device)\n",
    "        loss=criterion(output,target)\n",
    "        losses.append(loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr 0.0001:   0%|          | 0/4064 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bashturtle/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "Epoch 1, lr 0.0001:   2%|▏         | 72/4064 [00:12<08:51,  7.51it/s, loss=1.78076] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7e5f3aea6c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcurrent_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9caae88388ca>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#call called if parameters are given to the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdice_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2051\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#################TRAINING FUNCTION########################\n",
    "print('hello')\n",
    "batch_size=4\n",
    "n_epochs=52\n",
    "lr= 0.0001\n",
    "report_loss_threshold=10\n",
    "optimizer= Adam(model.parameters(),lr=lr)\n",
    "#criterion=loss\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    lr=lr\n",
    "    optimizer=Adam(model.parameters(),lr=lr)\n",
    "    model.train()\n",
    "    #define the progressbar here\n",
    "    tq=tqdm(total=len(train_loader)*batch_size)\n",
    "    tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n",
    "    losses=[]\n",
    "    #main training loop comes here\n",
    "    \n",
    "    for i,(inputs,target) in enumerate(train_loader):\n",
    "        #inputs=inputs.to(device)\n",
    "        target=target.to(device)\n",
    "        output=model(inputs).to(device)\n",
    "        loss=criterion(output,target)\n",
    "        optimizer.zero_grad()\n",
    "        current_batch_size=inputs.size(0)\n",
    "        tq.update(current_batch_size)\n",
    "        #print(loss.item())\n",
    "        losses.append(loss.item())\n",
    "        mean_loss=np.mean(losses[-report_loss_threshold:])\n",
    "        #set the postfix of the progress bar \n",
    "        tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n",
    "        (current_batch_size*loss).backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #close the progress bar \n",
    "    tq.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = list(range(100))\n",
    "with tqdm(total=len(my_list)) as pbar:\n",
    "    for x in my_list:\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
